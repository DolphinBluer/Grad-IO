\documentclass[xcolor=pdftex,dvipsnames,table,mathserif,aspectratio=169]{beamer}
\usetheme{metropolis}
%\usetheme{Darmstadt}
%\usepackage{times}
%\usefonttheme{structurebold}

\usepackage[english]{babel}
%\usepackage[table]{xcolor}
\usepackage{pgf,pgfarrows,pgfnodes,pgfautomata,pgfheaps}
\usepackage{amsmath,amssymb,setspace,outline}
\usepackage[latin1]{inputenc}
\usepackage[T1]{fontenc}
\usepackage{relsize}
\DeclareMathSizes{10}{10}{6}{6} 




\newcommand{\norm}[1]{\left\lVert#1\right\rVert}
\newcommand{\R}{\mathbb{R}}
\newcommand{\E}{\mathbb{E}}
\newcommand{\V}{\mathbb{V}}
\newcommand{\ol}{\overline}
%\newcommand{\ul}{\underline}
\newcommand{\pp}{{\prime \prime}}
\newcommand{\ppp}{{\prime \prime \prime}}
\newcommand{\policy}{\gamma}


\newcommand{\fp}{\frame[plain]}


\title{Bonus Lecture: Solving Systems of Equations}
\author{Chris Conlon  }
\institute{Grad IO}
\date{\today }
\setbeamerfont{equation}{size=\tiny}
\begin{document}

\begin{frame}
\titlepage
\end{frame}

\begin{frame}{Basic Setup}
Often we are interested in solving a problem like this:
\begin{description}
\item[Root Finding] $f(x) = 0 $
\item[Optimization] $\arg \min_x f(x)$.
\end{description}
These problems are related because we find the minimum by setting: $f'(x)=0$
\end{frame}

\section{Root Finding} 

\begin{frame}{Newton's Method for Root Finding}
Consider the Taylor series for $f(x)$ approximated around $f(x_0)$:
\begin{align*}
f(x) \approx f(x_0) + f'(x_0) \cdot (x-x_0) + f''(x_0) \cdot (x-x_0)^2 + o_p(3)
\end{align*}
Suppose we wanted to find a \alert{root} of the equation where $f(x^{*})=0$ and solve for $x$:
\begin{align*}
0 &= f(x_0) + f'(x_0) \cdot (x-x_0) \\
x_1 &= x_0-\frac{f(x_0)}{f'(x_0)} 
\end{align*}
This gives us an \alert{iterative} scheme to find $x^{*}$:
\begin{enumerate}
\item Start with some $x_k$. Calculate $f(x_k),f'(x_k)$
\item Update using $x_{k+1} = x_k - \frac{f(x_k)}{f'(x_k)} $
\item Stop when $|x_{k+1}-x_{k}| < \epsilon_{tol}$.
\end{enumerate}
\end{frame}

\begin{frame}{Halley's Method for Root Finding}
Consider the Taylor series for $f(x)$ approximated around $f(x_0)$:
\begin{align*}
f(x) \approx f(x_0) + f'(x_0) \cdot (x-x_0) + f''(x_0) \cdot (x-x_0)^2 + o_p(3)
\end{align*}
Now let's consider the second-order approximation:
\begin{align*}
x_{n+1}
&=x_{n}-\frac{2 f\left(x_{n}\right) f^{\prime}\left(x_{n}\right)}{2\left[f^{\prime}\left(x_{n}\right)\right]^{2}-f\left(x_{n}\right) f^{\prime \prime}\left(x_{n}\right)}
=x_{n}-\frac{f\left(x_{n}\right)}{f^{\prime}\left(x_{n}\right)-\frac{f\left(x_{n}\right)}{f^{\prime}\left(x_{n}\right)} \frac{f^{\prime \prime}\left(x_{n}\right)}{2}}\\
&=x_{n}-\frac{f\left(x_{n}\right)}{f^{\prime}\left(x_{n}\right)}\left[1-\frac{f\left(x_{n}\right)}{f^{\prime}\left(x_{n}\right)} \cdot \frac{f^{\prime \prime}\left(x_{n}\right)}{2 f^{\prime}\left(x_{n}\right)}\right]^{-1}
\end{align*}
\vspace{-.4cm}
\begin{itemize}
\item Last equation is useful because we only need to know $f(x_n)/f'(x_n)$ and $f''(x_n)/f'(x_n)$
\item If we are lucky $f''(x_n)/f'(x_n)$ is easy to compute or $\approx 0$ (Newton's method).
\end{itemize}
\end{frame}

\begin{frame}{Root Finding: Convergence}
How many iterations do we need? This is a tough question to answer.
\begin{itemize}
\item However we can consider convergence where $f(a) =0$:
\begin{align*}
\left|x_{n+1}-a\right| \leq K_d *\left|x_{n}-a\right|^{d}
\end{align*}
\begin{itemize}
\item $d=2$ (Newton's Method) \alert{quadratic convergence}  (we need $f'(x)$)
\item $d=3$ (Halley's Method) \alert{cubic convergence} (but we need $f''(x)$)
\end{itemize}
\end{itemize}
\end{frame}

\begin{frame}{Root Finding: Fixed Points}
Some (not all) equations can be written as $f(x) = x$ or $g(x)=0: f(x)-x =0$.
\begin{itemize}
\item In this case we can iterate on the \alert{fixed point} directly
\begin{align*}
x_{n+1} = f(x_n)
\end{align*}
\item Advantage: we only need to calculate $f(x)$.
\item There need not be a unique solution to $f(x) = x$.
\item But... this may or may not actually work.
\end{itemize}
\end{frame}

\begin{frame}{Contraction Mapping Theorem/ Banach Fixed Point}
\small
Consider a set $D \subset \mathbb{R}^{n}$ and a function $f: D \rightarrow \mathbb{R}^{n} .$ Assume
\begin{enumerate}
\item $D$ is closed (i.e., it contains all limit points of sequences in $D$ )
\item $x \in D \Longrightarrow f(x) \in D$
\item The mapping $g$ is a contraction on $D:$ There exists $q<1$ such that
\begin{align*}
\forall x, y \in D: \quad\|f(x)-f(y)\| \leq q\|x-y\|
\end{align*}
\noindent Then \vspace{-.3cm}
\end{enumerate}
\begin{enumerate}
\item There exists a unique  $x^{*} \in D$ with $f\left(x^{*}\right)=x^{*}$
\item For any $x^{(0)} \in D$ the fixed point iterates given by $x^{(k+1)}:=f\left(x^{(k)}\right)$ converge to $x^{*}$ as $k \rightarrow \infty$
\item $x^{(k)}$ satisfies the \alert{a-priori error} estimate $\left\|x^{(k)}-x^{*}\right\| \leq \frac{q^{k}}{1-q}\left\|x^{(1)}-x^{(0)}\right\|$
\item $x^{(k)}$ satisfies the \alert{a-posteriori error} estimate $\left\|x^{(k)}-x^{*}\right\| \leq \frac{q}{1-q}\left\|x^{(k)}-x^{(k-1)}\right\|$
\end{enumerate}
\end{frame}



\begin{frame}{Some notes}
\begin{itemize}
\item Not every fixed point relationship is a contraction.
\item Iterating on $x_{n+1} = f(x_n)$ will not always lead to $f(x) = x$ or $g(x) =0$.
\item Convergence rate of fixed point iteration is \alert{slow} or $q-$linear.
\item When $q$ is small this will be faster.
\item $q$ is sometimes called \alert{modulus} of contraction mapping.
\item A key example of a contraction: \alert{value function iteration}!
\end{itemize}
\end{frame}

\begin{frame}{Accelerated Fixed Points: Secant Method}
Start with Newton's method and use the finite difference approximation
\begin{align*}
f^{\prime}\left(x_{n-1}\right)  &\approx \frac{f\left(x_{n-1}\right)-f\left(x_{n-2}\right)}{x_{n-1}-x_{n-2}} \\
x_{n}&=x_{n-1}-f\left(x_{n-1}\right) \frac{x_{n-1}-x_{n-2}}{f\left(x_{n-1}\right)-f\left(x_{n-2}\right)}
\end{align*}
\begin{itemize}
\item This doesn't have the actual $f'(x_n)$ so it isn't quadratically convergent
\item Instead is is superlinear with rate $q = \frac{1 + \sqrt{5}}{2}=1.618 < 2$ (Golden Ratio)
\item Faster than fixed-point iteration but doesn't require computing $f'(x_n)$.
\item Idea: can use past iterations to approximate derivatives and accelerate fixed points.
\end{itemize}
\end{frame}

\begin{frame}{Accelerated Fixed Points: Anderson (1965) Mixing}
Define the residual $r(x_n) = f(x_n) - x_n$. Find weights on previous $k$ residuals:
\begin{align*}
\widehat{\alpha^{n}} &= \arg \min_{\alpha} \left\|\sum_{k=0}^{m} \alpha_{k}^{n} \cdot r_{n-k}\right\| \text { subject to } \sum_{k=0}^{m} \alpha_{k}^{n}=1\\
x_{n+1}&=\left(1-\lambda\right) \sum_{j=0}^{m} \widehat{\alpha_{k}^{n}}\cdot x_{n-k}+\lambda \sum_{j=0}^{m}\widehat{\alpha_{k}^{n}}\cdot f\left(x_{n-k}\right)
\end{align*}
\begin{itemize}
\item Convex combination of weighted average of: lagged $x_{n-k}$ and lagged $f(x_{n-k})$.
\item Variants on this are known as \alert{Anderson Mixing} or \alert{Anderson Acceleration}.
\end{itemize}
\end{frame}

\begin{frame}{Accelerated Fixed Points: SQUAREM (Varadhan and Roland 2008)}
Define the residual $r(x_n) = f(x_n) - x_n$ and $v(x_n)=f \circ r \left(x_{n}\right)=f \circ f \left(x_{n}\right)-f\left(x_{n}\right)$.
\begin{align*}
x_{n+1}=& x_{n}&-&2 s\left[f\left(x_{n}\right)-x_{n}\right] &+&s^{2}\left[f \circ f\left(x_{n}\right)-2 f\left(x_{n}\right)+x_{n}\right] \\
=& x_{n}&-&2 s r &+&s^{2}(v-r)
\end{align*}
Three versions of stepsize:
\begin{align*}
s_1 =\frac{r^{t} r}{r^{t}(v-r)}, \quad
s_2 =\frac{r^{t}(v-r)}{(v-r)^{t}(v-r)}, \quad
s_3 =-\sqrt{\frac{r^{t} r}{(v-r)^{t}(v-r)}}
\end{align*}
Idea: use two iterations to construct something more like the quadratic/Halley method.\\
\alert{Note: I am hand-waving, don't try to derive this.}
\end{frame}





\end{document}


